{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64aef73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import fsspec\n",
    "from pathlib import Path\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d39de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1080261/2194816378.py:4: FutureWarning: In a future version, xarray will not decode the variable 'step' into a timedelta64 dtype based on the presence of a timedelta-like 'units' attribute by default. Instead it will rely on the presence of a timedelta64 'dtype' attribute, which is now xarray's default way of encoding timedelta64 values.\n",
      "To continue decoding into a timedelta64 dtype, either set `decode_timedelta=True` when opening this dataset, or add the attribute `dtype='timedelta64[ns]'` to this variable on disk.\n",
      "To opt-in to future behavior, set `decode_timedelta=False`.\n",
      "  reanalysis = xr.open_zarr(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 1PB\n",
      "Dimensions:     (time: 1323648, hybrid: 137, values: 410240)\n",
      "Coordinates:\n",
      "  * time        (time) datetime64[ns] 11MB 1900-01-01 ... 2050-12-31T23:00:00\n",
      "  * hybrid      (hybrid) float64 1kB 1.0 2.0 3.0 4.0 ... 134.0 135.0 136.0 137.0\n",
      "    step        timedelta64[ns] 8B ...\n",
      "    valid_time  (time) datetime64[ns] 11MB dask.array<chunksize=(1323648,), meta=np.ndarray>\n",
      "Dimensions without coordinates: values\n",
      "Data variables:\n",
      "    d           (time, hybrid, values) float32 298TB dask.array<chunksize=(1, 1, 410240), meta=np.ndarray>\n",
      "    t           (time, hybrid, values) float32 298TB dask.array<chunksize=(1, 1, 410240), meta=np.ndarray>\n",
      "    vo          (time, hybrid, values) float32 298TB dask.array<chunksize=(1, 1, 410240), meta=np.ndarray>\n",
      "    w           (time, hybrid, values) float32 298TB dask.array<chunksize=(1, 1, 410240), meta=np.ndarray>\n",
      "Attributes: (12/14)\n",
      "    Conventions:               CF-1.7\n",
      "    GRIB_centre:               ecmf\n",
      "    GRIB_centreDescription:    European Centre for Medium-Range Weather Forec...\n",
      "    GRIB_edition:              2\n",
      "    GRIB_subCentre:            0\n",
      "    history:                   2022-10-09T17:47 GRIB to CDM+CF via cfgrib-0.9...\n",
      "    ...                        ...\n",
      "    pangeo-forge:recipe_hash:  ed5fbf407898def1e27d4e8965c2cdaa01b7ea8f2ac2e1...\n",
      "    pangeo-forge:version:      0.9.1\n",
      "    valid_time_start:          1940-01-01\n",
      "    last_updated:              2026-01-16 01:58:25.664990+00:00\n",
      "    valid_time_stop:           2025-10-31\n",
      "    valid_time_stop_era5t:     2026-01-10\n"
     ]
    }
   ],
   "source": [
    "fs = fsspec.filesystem('gs', token='anon')\n",
    "fs.ls('gs://gcp-public-data-arco-era5/co')\n",
    "\n",
    "reanalysis = xr.open_zarr(\n",
    "    fs.get_mapper('gs://gcp-public-data-arco-era5/co/single-level-reanalysis.zarr'),\n",
    "    consolidated=True,\n",
    ")\n",
    "\n",
    "print(reanalysis.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beffda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regridder_from_reduced_da(da, out_lon, out_lat):\n",
    "    \"\"\"\n",
    "    da: xarray DataArray with dims (..., values) and coords latitude(values), longitude(values)\n",
    "        (like ERA5 reduced Gaussian exposed by cfgrib)\n",
    "    out_lon: 1D array of target longitudes (e.g. 0..360)\n",
    "    out_lat: 1D array of target latitudes  (e.g. -90..90)\n",
    "\n",
    "    Returns:\n",
    "      regrid(values_1d) -> 2D array with shape (len(out_lat), len(out_lon))\n",
    "    \"\"\"\n",
    "    # Source points (1D, aligned with 'values' dim)\n",
    "    src_lon = np.asarray(da.longitude.values, dtype=np.float64)\n",
    "    src_lat = np.asarray(da.latitude.values, dtype=np.float64)\n",
    "\n",
    "    # Mirror lon==0 -> 360 to avoid seam artifacts near the dateline\n",
    "    # (works when you target [0, 360], inclusive)\n",
    "    m = np.isclose(src_lon, 0.0)\n",
    "    src_lon2 = np.concatenate([src_lon, src_lon[m] + 360.0])\n",
    "    src_lat2 = np.concatenate([src_lat, src_lat[m]])\n",
    "\n",
    "    # Delaunay triangulation in (lon, lat)\n",
    "    tri = scipy.spatial.Delaunay(np.stack([src_lon2, src_lat2], axis=1))\n",
    "\n",
    "    # Target mesh points: (N, 2) list of lon/lat pairs\n",
    "    # We'll output as (lat, lon) for typical image/array conventions.\n",
    "    Lon2d, Lat2d = np.meshgrid(out_lon, out_lat, indexing=\"xy\")\n",
    "    mesh = np.stack([Lon2d.ravel(), Lat2d.ravel()], axis=1)\n",
    "\n",
    "    # Precompute simplex + barycentric weights once (this is the big win)\n",
    "    simplex = tri.find_simplex(mesh)\n",
    "    X = mesh\n",
    "    T = tri.transform\n",
    "\n",
    "    ndim = 2\n",
    "    T_inv = T[simplex, :ndim, :]              # (N,2,2)\n",
    "    r = T[simplex, ndim, :]                   # (N,2)\n",
    "    c = np.einsum(\"nij,nj->ni\", T_inv, X - r) # (N,2)\n",
    "    w = np.concatenate([c, 1.0 - c.sum(axis=1, keepdims=True)], axis=1)  # (N,3)\n",
    "\n",
    "    verts = tri.simplices[simplex]            # (N,3)\n",
    "    valid = simplex != -1\n",
    "\n",
    "    nlat = len(out_lat)\n",
    "    nlon = len(out_lon)\n",
    "\n",
    "    def regrid(values_1d):\n",
    "        v = np.asarray(values_1d, dtype=np.float32)\n",
    "        # apply the same mirror as we did for coords\n",
    "        v2 = np.concatenate([v, v[m]])\n",
    "\n",
    "        out = np.full(mesh.shape[0], np.nan, dtype=np.float32)\n",
    "        vv = v2[verts[valid]]                 # (Nv,3)\n",
    "        out[valid] = np.einsum(\"ni,ni->n\", vv, w[valid]).astype(np.float32)\n",
    "\n",
    "        return out.reshape(nlat, nlon)        # (lat, lon)\n",
    "\n",
    "    return regrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ddfab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target regular grid (0.25°)\n",
    "out_lon = np.linspace(0, 360, 360*4 + 1)      # includes 360\n",
    "out_lat = np.linspace(-90, 90, 180*4 + 1)\n",
    "\n",
    "# build once from any one slice (no time slice needed if coords are time-invariant)\n",
    "sample = reanalysis[\"t2m\"].isel(time=0)\n",
    "regrid = build_regridder_from_reduced_da(sample, out_lon, out_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44505732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1059544/318842967.py:1: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  for t in pd.date_range(\"2020-01-01T00:00:00\", periods=24, freq=\"H\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 00:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 01:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 02:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 03:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 04:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 05:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 06:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 07:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 08:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 09:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 10:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 11:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 12:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 13:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 14:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 15:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 16:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 17:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 18:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 19:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 20:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 21:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 22:00:00 (542080,) (721, 1441)\n",
      "2020-01-01 23:00:00 (542080,) (721, 1441)\n"
     ]
    }
   ],
   "source": [
    "for t in pd.date_range(\"2020-01-01T00:00:00\", periods=24, freq=\"H\"):\n",
    "    one_hour = reanalysis[\"t2m\"].sel(time=t)\n",
    "    arr_1d = one_hour.values                  # (542080,)\n",
    "    arr_ll = regrid(arr_1d)                   # (721, 1441) for 0.25° with endpoints\n",
    "    print(t, arr_1d.shape, arr_ll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6c1def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 18.46 s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    reanalysis[\"t2m\"].sel(\n",
    "        time=slice(\"2020-01-01T00:00:00\", \"2020-01-07T23:00:00\")\n",
    "    ).to_netcdf(\n",
    "        \"./test_outputs/t2m_2020_H1_1.nc\",\n",
    "        compute=True\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
